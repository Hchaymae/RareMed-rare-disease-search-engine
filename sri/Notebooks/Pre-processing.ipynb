{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:21.349171Z",
     "start_time": "2024-12-06T08:50:21.345817Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddeb1114d5ffb53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:21.368195Z",
     "start_time": "2024-12-06T08:50:21.364657Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c455a83e406401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:21.985761Z",
     "start_time": "2024-12-06T08:50:21.984049Z"
    }
   },
   "outputs": [],
   "source": [
    "txt_directory = \"../Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1c2685c781833",
   "metadata": {},
   "source": [
    "<H1>Turn PDFs into rows of a DataFrame</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255be9f342f9936b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:22.499195Z",
     "start_time": "2024-12-06T08:50:22.497046Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078338a6f47676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:23.009566Z",
     "start_time": "2024-12-06T08:50:23.007180Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_texts_in_directory(directory_path):\n",
    "    result = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):  # Process only .txt files\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            content = extract_text(file_path)\n",
    "            if content is not None:\n",
    "                # Append file name (without extension) and content to the data list\n",
    "                result.append({\"title\": os.path.splitext(filename)[0], \"content\": content})\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c227ee6083ed3f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:23.523562Z",
     "start_time": "2024-12-06T08:50:23.517545Z"
    }
   },
   "outputs": [],
   "source": [
    "data = process_texts_in_directory(txt_directory)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9ee2489bb5011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:24.053945Z",
     "start_time": "2024-12-06T08:50:24.051706Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data.iloc[0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a00452dd7a5b6a",
   "metadata": {},
   "source": [
    "<H1>Tokenisation with Stop Word Removal</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32946063ba89508b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:24.086403Z",
     "start_time": "2024-12-06T08:50:24.082626Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # Convert to lowercase (ASCII is different)\n",
    "    text = text.lower()\n",
    "    # Remove newlines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Remove special characters like © and non-breaking spaces (shows up as NBSP)\n",
    "    text = text.replace('©', '').replace('\\u00A0', ' ')\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove references in square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Tokenise\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_words = [\n",
    "        word for word in words\n",
    "        if word not in stop_words  # Remove stopwords\n",
    "        and len(word) > 2         # Remove two-letter words\n",
    "        and not any(char in word for char in ['β', 'α', 'µ', 'δ', 'γ'])  # Remove words that include these characters\n",
    "        and not (word.isdigit() and len(word) > 4)  # Remove numbers longer than 4 digits\n",
    "        and not (word.isdigit() and len(word) == 3)  # Remove numbers that are 3 digits long\n",
    "        and not (len(word) == 4 and word.isdigit() and word[0] != '2')  # Remove 4-digit numbers not starting with '2' i.e. not recent dates, usually page numbers\n",
    "        and not re.search(r'^[a-zA-Z]*\\d+[a-zA-Z]+|[a-zA-Z]+\\d+[a-zA-Z]*$', word)  # Remove words that mix letters and digits\n",
    "        ]\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788a59b687fb202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:24.615448Z",
     "start_time": "2024-12-06T08:50:24.099864Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_data = data.copy()\n",
    "tokenized_data[\"tokens\"] = tokenized_data['content'].astype(str).apply(tokenize_text)\n",
    "tokenized_data.drop(columns=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fd50a1ae3bca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:25.124351Z",
     "start_time": "2024-12-06T08:50:25.119660Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd45c38ad6942d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:25.739033Z",
     "start_time": "2024-12-06T08:50:25.736653Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tokenized_data['tokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6e1b3f81384c8",
   "metadata": {},
   "source": [
    "<H1>Lemmatize</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e167346e5b6d12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:25.769339Z",
     "start_time": "2024-12-06T08:50:25.767164Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='n') for word in words]  # Lemmatize as nouns\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4494bf34b0eaf71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:25.965700Z",
     "start_time": "2024-12-06T08:50:25.781323Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_data = tokenized_data.copy()\n",
    "lemmatized_data[\"tokens_lemmatized\"] = lemmatized_data[\"tokens\"].apply(lambda x: lemmatize_words(x))\n",
    "lemmatized_data[\"text_for_tfidf\"] = lemmatized_data[\"tokens_lemmatized\"].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb5bb0d3fc46d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:26.494955Z",
     "start_time": "2024-12-06T08:50:26.488821Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8467f1bda34ddac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:27.025234Z",
     "start_time": "2024-12-06T08:50:27.023001Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lemmatized_data['tokens_lemmatized'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35329061afc3a0c3",
   "metadata": {},
   "source": [
    "<H1>Apply TF-IDF</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bfe112cc89eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:27.058312Z",
     "start_time": "2024-12-06T08:50:27.055849Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_tfidf(dataframe):\n",
    "    # Initialize TF-IDF Vectorizer with tokens as input\n",
    "    vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    # Fit and transform the tokenized content to a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(dataframe['tokens_lemmatized'])\n",
    "    # Convert the matrix to a DataFrame for easier viewing\n",
    "    result = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return result, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787478403b1bc18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:27.105205Z",
     "start_time": "2024-12-06T08:50:27.069442Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df, tfidf_vectorizer = apply_tfidf(lemmatized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c32f9e1b084248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:27.818545Z",
     "start_time": "2024-12-06T08:50:27.611726Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807cf0b8e5bd280",
   "metadata": {},
   "source": [
    "<H1>Create the Inverted Index</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1385090ef1051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:28.314094Z",
     "start_time": "2024-12-06T08:50:28.310116Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_inverted_index_from_tfidf(tfidf_df, tokenized_data):\n",
    "    # Step 1: Create a term dictionary (unique terms to termID mapping)\n",
    "    terms = tfidf_df.columns\n",
    "    term_dict = {idx: term for idx, term in enumerate(terms)}  # termID -> term\n",
    "\n",
    "    # Initialize list to build the inverted index\n",
    "    inverted_index = []\n",
    "\n",
    "    # Step 2: Build the inverted index (termID, docID, freq, tf-idf, positions)\n",
    "    for docID, tokens in enumerate(tokenized_data['tokens']):\n",
    "        term_freq = {}  # term -> frequency in the document\n",
    "        positions = {}  # term -> positions list\n",
    "        \n",
    "        for pos, term in enumerate(tokens):\n",
    "            termID = next((k for k, v in term_dict.items() if v == term), None)  # Get termID for the term\n",
    "            if termID is not None:  # If the term exists in the term_dict\n",
    "                if termID not in term_freq:\n",
    "                    term_freq[termID] = 0\n",
    "                    positions[termID] = []\n",
    "                term_freq[termID] += 1\n",
    "                positions[termID].append(pos)\n",
    "\n",
    "        # Add entries to the inverted index\n",
    "        for termID, freq in term_freq.items():\n",
    "            tfidf_score = tfidf_df.iloc[docID, termID]  # Get the TF-IDF score for the term in this document\n",
    "            inverted_index.append({\n",
    "                'termID': termID,\n",
    "                'docID': docID,\n",
    "                'freq': freq,\n",
    "                'tf-idf': tfidf_score,\n",
    "                'positions': positions[termID]\n",
    "            })\n",
    "\n",
    "    # Step 3: Create DataFrames for the results\n",
    "    result_term_dict = pd.DataFrame(list(term_dict.items()), columns=['termID', 'term'])\n",
    "    result_inverted_index = pd.DataFrame(inverted_index)\n",
    "\n",
    "    return result_term_dict, result_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bad09a9b1d706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:41.992940Z",
     "start_time": "2024-12-06T08:50:28.808766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "term_dict_df, inverted_index_df = create_inverted_index_from_tfidf(tfidf_df, tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31945f7a6477423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:42.487809Z",
     "start_time": "2024-12-06T08:50:42.484013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Viewing the results\n",
    "print(\"Term Dictionary:\")\n",
    "print(term_dict_df.head())\n",
    "print(\"\\nInverted Index:\")\n",
    "print(inverted_index_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b7f9d3cf1cae6",
   "metadata": {},
   "source": [
    "<H1>Information Retrieval Model</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028310b4314c42c",
   "metadata": {},
   "source": [
    "## Query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0a9e40f815387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:42.990917Z",
     "start_time": "2024-12-06T08:50:42.989136Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"tracheal rupture\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef6f0b14fee7a7",
   "metadata": {},
   "source": [
    "ps this is only to show, skip and  use the function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3559fd506444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:43.493009Z",
     "start_time": "2024-12-06T08:50:43.490529Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_query = tokenize_text(query)\n",
    "print(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcdc912d6aac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:43.992611Z",
     "start_time": "2024-12-06T08:50:43.990479Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_query = lemmatize_words(tokenized_query)\n",
    "print(lemmatized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131cf1399c4e843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:44.589703Z",
     "start_time": "2024-12-06T08:50:44.586899Z"
    }
   },
   "outputs": [],
   "source": [
    "def match_query_to_documents(query_string, tfidf_data, vectorizer, top_n=5):\n",
    "    # Step 1: Preprocess the query\n",
    "    processed_query = tokenize_text(query_string)\n",
    "    processed_query = lemmatize_words(processed_query)\n",
    "    \n",
    "    # Step 2: Transform the query into the same TF-IDF space as documents\n",
    "    query_tfidf = vectorizer.transform([processed_query])  # Query as a 1xN vector    \n",
    "    if np.count_nonzero(query_tfidf.toarray()) == 0:\n",
    "        print(\"Query vector is zero. Check query preprocessing.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Compute cosine similarity between query and documents\n",
    "    cosine_sim = cosine_similarity(query_tfidf, tfidf_data)\n",
    "    \n",
    "    # Step 4: Get the most similar document(s)\n",
    "    similarity_scores = cosine_sim.flatten()\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]  # Get indices of top N scores (descending order)\n",
    "    top_scores = similarity_scores[top_indices]  # Corresponding scores\n",
    "    \n",
    "    return top_indices, top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f5159371a0af6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:45.091282Z",
     "start_time": "2024-12-06T08:50:45.085999Z"
    }
   },
   "outputs": [],
   "source": [
    "top_indices, top_scores = match_query_to_documents(query, tfidf_df, tfidf_vectorizer, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80050ef7c2a622dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:45.590119Z",
     "start_time": "2024-12-06T08:50:45.587769Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Top 5 matching documents:\")\n",
    "for idx, score in zip(top_indices, top_scores):\n",
    "    print(f\"Document Title: {data.iloc[idx]['title']}, Similarity Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5746a885bf013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:46.150183Z",
     "start_time": "2024-12-06T08:50:46.089904Z"
    }
   },
   "outputs": [],
   "source": [
    "term_dict_df.to_csv(\"term_dict.csv\", index=False)\n",
    "inverted_index_df.to_csv(\"inverted_index.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d44289af336a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:50:46.648519Z",
     "start_time": "2024-12-06T08:50:46.646008Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_list = tokenized_data.drop(columns=['tokens'])\n",
    "doc_list.to_csv(\"doc_list.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236537708d083992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
